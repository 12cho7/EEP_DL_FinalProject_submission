{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Load and Use Trained Model\n",
    "\n",
    "This notebook demonstrates how to load the trained model and make predictions.\n",
    "\n",
    "## Pre-trained Model Link\n",
    "\n",
    "The trained model (v3.0) is available at:\n",
    "- **Google Drive**: [Download Link](YOUR_GOOGLE_DRIVE_LINK) - Update this link after uploading\n",
    "- **Hugging Face Hub**: [Model Card](https://huggingface.co/YOUR_USERNAME/llm-response-comparison-v3) - Optional\n",
    "\n",
    "### Model Details\n",
    "- **Base Model**: microsoft/deberta-v3-base\n",
    "- **Fine-tuning Method**: LoRA (r=16, alpha=32)\n",
    "- **Validation Log Loss**: 1.0735\n",
    "- **Validation Accuracy**: 39.78%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers peft accelerate sentencepiece protobuf -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from peft import PeftModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "MAX_LENGTH = 1280\n",
    "NUM_LABELS = 3\n",
    "\n",
    "# Model path - Update this to your model location\n",
    "# Option 1: Local path (if downloaded)\n",
    "MODEL_PATH = 'checkpoints/models_v3'\n",
    "\n",
    "# Option 2: Google Drive (if uploaded to Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# MODEL_PATH = '/content/drive/MyDrive/models_v3'\n",
    "\n",
    "# Option 3: Hugging Face Hub (if uploaded)\n",
    "# MODEL_PATH = 'YOUR_USERNAME/llm-response-comparison-v3'\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"✓ Tokenizer loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "config.num_labels = NUM_LABELS\n",
    "config.problem_type = \"single_label_classification\"\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, config=config\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"✓ Model loaded successfully\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def parse_json(text):\n",
    "    \"\"\"Parse JSON text safely\"\"\"\n",
    "    try:\n",
    "        p = json.loads(text) if isinstance(text, str) else text\n",
    "        return '\\n'.join([str(i) for i in p]) if isinstance(p, list) else str(p)\n",
    "    except:\n",
    "        return str(text)\n",
    "\n",
    "def truncate(text, max_chars):\n",
    "    \"\"\"Smart truncation: keep head and tail\"\"\"\n",
    "    if len(text) <= max_chars:\n",
    "        return text\n",
    "    h = int(max_chars * 0.6)\n",
    "    t = max_chars - h - 10\n",
    "    return text[:h] + \"\\n[...]\\n\" + text[-t:]\n",
    "\n",
    "def predict_single(prompt, response_a, response_b, model, tokenizer, max_length=1280):\n",
    "    \"\"\"\n",
    "    Predict which response is better for a single example.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The question/prompt text\n",
    "        response_a: First response\n",
    "        response_b: Second response\n",
    "        model: Loaded model\n",
    "        tokenizer: Loaded tokenizer\n",
    "        max_length: Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        dict with probabilities for A wins, B wins, and tie\n",
    "    \"\"\"\n",
    "    max_chars = (max_length * 4) // 3\n",
    "    \n",
    "    prompt_text = truncate(parse_json(prompt), max_chars // 4)\n",
    "    resp_a = truncate(parse_json(response_a), max_chars * 3 // 8)\n",
    "    resp_b = truncate(parse_json(response_b), max_chars * 3 // 8)\n",
    "    \n",
    "    text = f\"Compare responses:\\n\\nQ: {prompt_text}\\n\\n[A]: {resp_a}\\n\\n[B]: {resp_b}\\n\\nBetter?\"\n",
    "    \n",
    "    enc = tokenizer(text, truncation=True, padding='max_length', \n",
    "                    max_length=max_length, return_tensors='pt')\n",
    "    \n",
    "    input_ids = enc['input_ids'].to(model.device)\n",
    "    attention_mask = enc['attention_mask'].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "    \n",
    "    probs = np.clip(probs, 1e-7, 1 - 1e-7)\n",
    "    probs = probs / probs.sum()\n",
    "    \n",
    "    return {\n",
    "        'winner_model_a': float(probs[0]),\n",
    "        'winner_model_b': float(probs[1]),\n",
    "        'winner_tie': float(probs[2])\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Single prediction\n",
    "prompt = \"What is machine learning?\"\n",
    "response_a = \"Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It involves algorithms that can identify patterns and make decisions based on data.\"\n",
    "response_b = \"Machine learning is when computers learn stuff from data.\"\n",
    "\n",
    "result = predict_single(prompt, response_a, response_b, model, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Prediction Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nResponse A: {response_a[:100]}...\")\n",
    "print(f\"Response B: {response_b}\")\n",
    "print(f\"\\nProbabilities:\")\n",
    "print(f\"  A wins: {result['winner_model_a']:.4f} ({result['winner_model_a']*100:.2f}%)\")\n",
    "print(f\"  B wins: {result['winner_model_b']:.4f} ({result['winner_model_b']*100:.2f}%)\")\n",
    "print(f\"  Tie:    {result['winner_tie']:.4f} ({result['winner_tie']*100:.2f}%)\")\n",
    "\n",
    "winner = 'A' if result['winner_model_a'] > result['winner_model_b'] and result['winner_model_a'] > result['winner_tie'] else 'B' if result['winner_model_b'] > result['winner_tie'] else 'Tie'\n",
    "print(f\"\\n✓ Predicted winner: {winner}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Output\n",
    "\n",
    "After running the cells above, you should see:\n",
    "\n",
    "1. **Model loaded successfully** - Confirmation that the model is ready\n",
    "2. **Prediction results** - Probabilities for each class (A wins, B wins, Tie)\n",
    "3. **Predicted winner** - The class with highest probability\n",
    "\n",
    "The model can now be used for inference on new data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
